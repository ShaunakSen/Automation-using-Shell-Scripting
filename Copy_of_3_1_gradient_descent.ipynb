{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 3_1_gradient_descent.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaunakSen/Automation-using-Shell-Scripting/blob/master/Copy_of_3_1_gradient_descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "fd2c27ac75b58ab9cfdb05285bcf0292",
          "grade": false,
          "grade_id": "cell-9eeeb7abc468a506",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "UfVVzk9wazqm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 1: Gradient Descent"
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "7515d498d97561d9884b0802387b3be4",
          "grade": false,
          "grade_id": "cell-52980e134e2f9e19",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "DbNxvQhGazqn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this lab we will implement some of the optimisation methods we learned in the lecture. First, we will start by revisiting gradient descent for linear regression. However, in this implementation we will observe how the model parameters are updated over iterations of the gradient descent algorithm. \n",
        "\n",
        "Let's start by implementing gradient descent on a simple linear regression dataset, like the one you generated in Lab 1, but this time shifted so that it ranges from -5 to 5."
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "ca9cf8bef763c9249a69f3120f9749b4",
          "grade": true,
          "grade_id": "cell-02c39c20df2f2d24",
          "locked": false,
          "points": 1,
          "schema_version": 1,
          "solution": true
        },
        "id": "4oYv9NjNazqp",
        "colab_type": "code",
        "outputId": "f2a58ce5-7ff6-46c6-8d16-89dbe0e7a648",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "## generate M data points roughly forming a line (noise added)\n",
        "M = 100\n",
        "theta_true = torch.Tensor([[0.5], [2]])\n",
        "\n",
        "\n",
        "X = 10 * torch.rand(M, 2) - 5\n",
        "X[:, 1] = 1.0\n",
        "\n",
        "# print(X.shape)\n",
        "\n",
        "y = torch.mm(X, theta_true) + 0.3 * torch.randn(M, 1)\n",
        "\n",
        "# print(y.shape)\n",
        "\n",
        "# print(X[:,0].numpy())\n",
        "\n",
        "# print(y.numpy().reshape(1,100)[0])\n",
        "\n",
        "plt.scatter(X[:,0].numpy(), y.numpy().reshape(1,100)[0]);\n",
        "plt.xlabel(\"X\");\n",
        "plt.ylabel(\"y\");\n",
        "\n",
        "## visualise the data by plotting it\n",
        "# YOUR CODE HERE\n",
        "# raise NotImplementedError()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFYCAYAAABpkTT0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnAEAMAzhDACAYQhnAAAM8/8ByL853L2AhaEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "0b22b69e67951075996148ee0cb75f26",
          "grade": false,
          "grade_id": "cell-4c65a1279417a414",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "0Y_HyeZgazqs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You should now have data points according to y = mx + b where m = theta_true[0,0] and b = theta_true[1,0]. Note, $m = \\theta_1$ and $b = \\theta_0$.\n",
        "\n",
        "Now, let's implement gradient descent using the Mean Squared Error (MSE) cost function. \n",
        "\n",
        "Recall that: \n",
        "\n",
        "$J(\\theta) = \\frac{1}{2 M} \\sum_{i = 1}^M (h_{\\theta} (x^{(i)}) - y^{(i)} )^2$\n",
        "\n",
        "for $i = 1 \\text{  to iters (or until convergence)}$ <br>\n",
        "\n",
        "$\\hspace{1cm} w_i \\leftarrow w_i - \\eta \\frac{\\partial J}{\\partial w_i}$\n",
        "\n",
        "Implement the functions below in order to plot the cost function as well as the weight updates over iterations of gradient descent."
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "f7f41955a715267d6d73e184b8b43388",
          "grade": true,
          "grade_id": "cell-a52e2455d84ff4da",
          "locked": false,
          "points": 5,
          "schema_version": 1,
          "solution": true
        },
        "id": "d5KKXaH2azqt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## hypothesis computes $h_theta$\n",
        "def hypothesis(theta, X):\n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "    return X.mm(theta)\n",
        "  \n",
        "    \n",
        "    # raise NotImplementedError()\n",
        "\n",
        "## grad_cost_func computes the gradient of J for linear regression given J is the MSE \n",
        "def grad_cost_func(theta, X, y): \n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "    M = X.shape[1]\n",
        "    \n",
        "    grad = 1/M * X.t().mm(hypothesis(theta, X) - y)\n",
        "    \n",
        "    # raise NotImplementedError()\n",
        "    \n",
        "    return grad\n",
        "\n",
        "## cost_func computes the cost function J\n",
        "def cost_func(theta, X, y): \n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "    # 2x1, 100x2, 100x1\n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    This func should accept a matrix where each row is a value of theta_0 and theta_1\n",
        "    \n",
        "    It should return a list of losses for such rows\n",
        "    \n",
        "    cost_func(this_theta,X,y)[0] <- only one loss value comes as we are computing loss for only one theta\n",
        "    \n",
        "    J_grid = cost_func(theta_grid.t(), X, y) : 100 loss values for each pair of theta value from theta_grid()\n",
        "    \n",
        "    This will help us plot the fig in RHS (for 100 vals of theta_1 what was the loss)\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    \n",
        "    M = X.shape[1]\n",
        "    \n",
        "    h_theta = hypothesis(theta, X)\n",
        "    \n",
        "    loss = 1/2*M * ((h_theta-y).t().mm(h_theta-y))\n",
        "    \n",
        "    return [loss]\n",
        "    \n",
        "    # raise NotImplementedError()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7J4S6SLBeZnt",
        "colab_type": "code",
        "outputId": "32544d5f-53f9-4238-9eea-e9b07b7d3390",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "print(X.shape, theta_true.shape, y.shape)\n",
        "\n",
        "print(X.mm(theta_true).shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 2]) torch.Size([2, 1]) torch.Size([100, 1])\n",
            "torch.Size([100, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "81607b0b07d30d5873cfc349f49a5000",
          "grade": false,
          "grade_id": "cell-d0cb37c31097d491",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "KNKJ78XFazqv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's plot the updates to see what is happening as we iterate over the algorithm. First, we will plot $J$ as a function of $\\theta_1$ as well as the resulting equation of the line learned over $N=5$ iterations. Once your code is working, modify the value of $\\eta$ to see how it affects convergence.\n",
        "\n",
        "The figure below illustrates what you're aiming to plot. Note, much of the code to generate the figures is given below, you mostly need to complete the 3 functions above and then fill in a few missing lines of code below."
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "0c8aa3bd5465cf5b8a6d2ad5976e69ff",
          "grade": false,
          "grade_id": "cell-32de56d64c670852",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "wOXjYE7Zazqv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img src=\"http://comp6248.ecs.soton.ac.uk/labs/lab3/Figure1.png\">"
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "b50e9a89855e23ac6ffc655323c55876",
          "grade": false,
          "grade_id": "cell-59662fa076965410",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "gdt513Fnazqw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### First generate the figure on the left hand side. This plot shows the data and the linear fit of the data as the model parameters change over the 5 iterations."
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "ba6c7a291b78ef143956304d0159ea52",
          "grade": true,
          "grade_id": "cell-7663cd58d1e91c37",
          "locked": false,
          "points": 2,
          "schema_version": 1,
          "solution": true
        },
        "id": "rtLNR0Toazqw",
        "colab_type": "code",
        "outputId": "20b64f1d-90f6-4e81-93ad-5c0eb7ece0d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        }
      },
      "cell_type": "code",
      "source": [
        "## Now we can plot the lines over iterations\n",
        "## To do this, we start by constructing a grid of parameter pairs and their corresponding cost function values. \n",
        "\n",
        "# plot 100 pts\n",
        "x_axis = np.linspace(-1,1,100)\n",
        "# print(x_axis)\n",
        "\n",
        "\n",
        "theta_grid = torch.Tensor(len(x_axis),2)\n",
        "\n",
        "# theta_grid shape: 100x2\n",
        "\n",
        "# fill 1st col with values from x_axis and 2nd col with 2\n",
        "\n",
        "theta_grid[:,0] = torch.from_numpy(x_axis)\n",
        "theta_grid[:,1] = 2.0\n",
        "\n",
        "J_grid = cost_func(theta_grid.t(), X, y)\n",
        "\n",
        "N = 5\n",
        "eta = 0.003\n",
        "\n",
        "theta_0 = torch.Tensor([[0.0], [2.0]]) #initialise \n",
        "J_t = torch.Tensor(1,N)\n",
        "theta = torch.Tensor(2,1,N)\n",
        "J_t[:,0] = cost_func(theta_0, X, y)[0]\n",
        "theta[:,:,0] = theta_0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for j in range(1,N):\n",
        "    last_theta = theta[:,:,j-1]\n",
        "    ## Compute the value of this_theta\n",
        "    # YOUR CODE HERE\n",
        "    # raise NotImplementedError()\n",
        "    \n",
        "    grad = grad_cost_func(last_theta, X, y)\n",
        "    \n",
        "    # print (\"Grad:\", grad)\n",
        "    \n",
        "    this_theta = last_theta - eta*grad\n",
        "    \n",
        "    \n",
        "    theta[:,:,j] = this_theta\n",
        "    J_t[:,j] = cost_func(this_theta,X,y)[0]\n",
        "    \n",
        "print(J_t, theta)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    \n",
        "colors = ['b', 'g', 'm', 'c', 'orange']\n",
        "\n",
        "## Plot the data \n",
        "# YOUR CODE HERE\n",
        "# raise NotImplementedError()\n",
        "\n",
        "plt.xlabel(r'$x$')\n",
        "plt.ylabel(r'$y$')\n",
        "plt.title('Data and fit')\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-1.         -0.97979798 -0.95959596 -0.93939394 -0.91919192 -0.8989899\n",
            " -0.87878788 -0.85858586 -0.83838384 -0.81818182 -0.7979798  -0.77777778\n",
            " -0.75757576 -0.73737374 -0.71717172 -0.6969697  -0.67676768 -0.65656566\n",
            " -0.63636364 -0.61616162 -0.5959596  -0.57575758 -0.55555556 -0.53535354\n",
            " -0.51515152 -0.49494949 -0.47474747 -0.45454545 -0.43434343 -0.41414141\n",
            " -0.39393939 -0.37373737 -0.35353535 -0.33333333 -0.31313131 -0.29292929\n",
            " -0.27272727 -0.25252525 -0.23232323 -0.21212121 -0.19191919 -0.17171717\n",
            " -0.15151515 -0.13131313 -0.11111111 -0.09090909 -0.07070707 -0.05050505\n",
            " -0.03030303 -0.01010101  0.01010101  0.03030303  0.05050505  0.07070707\n",
            "  0.09090909  0.11111111  0.13131313  0.15151515  0.17171717  0.19191919\n",
            "  0.21212121  0.23232323  0.25252525  0.27272727  0.29292929  0.31313131\n",
            "  0.33333333  0.35353535  0.37373737  0.39393939  0.41414141  0.43434343\n",
            "  0.45454545  0.47474747  0.49494949  0.51515152  0.53535354  0.55555556\n",
            "  0.57575758  0.5959596   0.61616162  0.63636364  0.65656566  0.67676768\n",
            "  0.6969697   0.71717172  0.73737374  0.75757576  0.77777778  0.7979798\n",
            "  0.81818182  0.83838384  0.85858586  0.87878788  0.8989899   0.91919192\n",
            "  0.93939394  0.95959596  0.97979798  1.        ]\n",
            "tensor([[160.1396,   8.0581,   8.0417,   8.0318,   8.0246]]) tensor([[[0.0000, 0.4763, 0.4785, 0.4786, 0.4787]],\n",
            "\n",
            "        [[2.0000, 2.0163, 2.0130, 2.0102, 2.0078]]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n    \\ncolors = ['b', 'g', 'm', 'c', 'orange']\\n\\n## Plot the data \\n# YOUR CODE HERE\\n# raise NotImplementedError()\\n\\nplt.xlabel(r'$x$')\\nplt.ylabel(r'$y$')\\nplt.title('Data and fit')\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "v6jQo5gIDKuN",
        "colab_type": "code",
        "outputId": "2ffd6db5-d299-48a4-c81b-0a5b8166ea6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "test = torch.Tensor(2, 1, 5)\n",
        "print(test)\n",
        "\n",
        "print(test[:,0,:])\n",
        "\n",
        "T_data = [[[1., 2.], \n",
        "           [3., 4.]],\n",
        "          [[5., 6.], \n",
        "           [7., 8.]],\n",
        "          [[9., 10.],\n",
        "           [11., 12.]]\n",
        "         ]\n",
        "\n",
        "\n",
        "T = torch.tensor(T_data)\n",
        "print(T.shape)\n",
        "\n",
        "print(T[2])\n",
        "\n",
        "theta = torch.Tensor(2,1,5)\n",
        "\n",
        "print(theta)\n",
        "\n",
        "print(theta[:,:,0])\n",
        "\n",
        "\"\"\"\n",
        "J = torch.Tensor(1,5)\n",
        "\n",
        "print(J)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[5.4926e-36, 0.0000e+00, 0.0000e+00, 0.0000e+00,        nan]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "eaa1ff5b63e1b744b02ea8afa8c533a5",
          "grade": false,
          "grade_id": "cell-e7fd81fa921c6327",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "QzQYC0_1azqy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Next, generate the plots on the right hand side. This figure is a plot of the cost function over the value of $\\theta_1$ as well as the updates of $\\theta_1$ over iterations."
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "af94209b76d722c3775a2015d1b90dbc",
          "grade": true,
          "grade_id": "cell-c9fa638485b0726e",
          "locked": false,
          "points": 1,
          "schema_version": 1,
          "solution": true
        },
        "id": "v3ZQpCGQazq0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "\n",
        "# add the plot axes labels and title\n",
        "plt.xlabel(r'$\\theta_1$')\n",
        "plt.ylabel(r'$J(\\theta_1)$')\n",
        "plt.title('Cost function')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "d06192ebc2f3cc31e5c1d42b924f1283",
          "grade": false,
          "grade_id": "cell-e70124241b132a09",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "djMgC7cpazq2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Finally, generate a contour plot of the cost function"
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "c5a3cbc10e84dcf6671cd5f1ff181839",
          "grade": true,
          "grade_id": "cell-83e4d7737cbda3d1",
          "locked": false,
          "points": 1,
          "schema_version": 1,
          "solution": true
        },
        "id": "W7TSbAYPazq2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Generate a grid of values for theta_0 and theta_1 and compute the cost function for every combination.\n",
        "\n",
        "theta_0_vals = np.linspace(-1.0,1,100)\n",
        "theta_1_vals = np.linspace(-4.0,4,100)\n",
        "theta = torch.Tensor(len(theta_0_vals),2)\n",
        "\n",
        "# Compute the cost function over every combination of values for theta in a variable called J which will then be plot below\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "\n",
        "xc,yc = np.meshgrid(theta_0_vals, theta_1_vals)\n",
        "contours = plt.contour(xc, yc, J, 20)\n",
        "plt.clabel(contours)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}